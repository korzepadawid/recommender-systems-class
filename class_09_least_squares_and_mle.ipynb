{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-certification",
   "metadata": {},
   "source": [
    "# Alternating Least Squares (ALS) and Maximum Likelihood Estimation (MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-tourist",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_ratings_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"ratings.csv\")).rename(columns={'userId': 'user_id', 'movieId': 'item_id'})\n",
    "ml_movies_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"movies.csv\")).rename(columns={'movieId': 'item_id'})\n",
    "ml_df = pd.merge(ml_ratings_df, ml_movies_df, on='item_id')\n",
    "\n",
    "# Filter the data to reduce the number of movies\n",
    "seed = 6789\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "left_ids = rng.choice(ml_movies_df['item_id'], size=90, replace=False)\n",
    "left_ids = list(set(left_ids).union(set([1, 318, 1193, 1208, 1214, 1721, 2959, 3578, 4306, 109487])))\n",
    "\n",
    "ml_ratings_df = ml_ratings_df.loc[ml_ratings_df['item_id'].isin(left_ids)]\n",
    "ml_movies_df = ml_movies_df.loc[ml_movies_df['item_id'].isin(left_ids)]\n",
    "ml_df = ml_df.loc[ml_df['item_id'].isin(left_ids)]\n",
    "\n",
    "display(ml_movies_df.head(10))\n",
    "\n",
    "print(\"Number of interactions left: {}\".format(len(ml_ratings_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-brooklyn",
   "metadata": {},
   "source": [
    "## Shift item ids and user ids so that they are consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = ml_ratings_df.copy()\n",
    "\n",
    "unique_item_ids = interactions_df['item_id'].unique()\n",
    "item_id_mapping = dict(zip(unique_item_ids, list(range(len(unique_item_ids)))))\n",
    "item_id_reverse_mapping = dict(zip(list(range(len(unique_item_ids))), unique_item_ids))\n",
    "unique_user_ids = interactions_df['user_id'].unique()\n",
    "user_id_mapping = dict(zip(unique_user_ids, list(range(len(unique_user_ids)))))\n",
    "user_id_reverse_mapping = dict(zip(list(range(len(unique_user_ids))), unique_user_ids))\n",
    "\n",
    "interactions_df['item_id'] = interactions_df['item_id'].map(item_id_mapping)\n",
    "interactions_df['user_id'] = interactions_df['user_id'].map(user_id_mapping)\n",
    "\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-meeting",
   "metadata": {},
   "source": [
    "## Get the number of items and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = np.max(interactions_df['item_id']) + 1\n",
    "n_users = np.max(interactions_df['user_id']) + 1\n",
    "\n",
    "print(\"n_items={}\\nn_users={}\".format(n_items, n_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-threshold",
   "metadata": {},
   "source": [
    "## Get the user-item interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-mexico",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mapping to int is necessary because of how iterrows works\n",
    "r = np.zeros(shape=(n_users, n_items))\n",
    "for idx, interaction in interactions_df.iterrows():\n",
    "    r[int(interaction['user_id'])][int(interaction['item_id'])] = 1\n",
    "    \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c79aa3",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622743b",
   "metadata": {},
   "source": [
    "**Task 1.** Write a method perform_least_squares for performing the matrix calculation of the linear model coefficients minimizing the square loss for a given 2D dataset (where the first column is x - the explanatory variable, the second column is y - the target variable). The method should return a tuple of fitted theta_0 and theta_1. To fit the bias term (theta_0) you have to concatenate 1 to every input vector, e.g. $[0.2]$ -> $[1, 0.2]$.\n",
    "\n",
    "The interface of the method should be as follows:\n",
    "    \n",
    "    perform_least_squares(data)\n",
    "    \n",
    "Compare your solution to the optimal one found by sklearn.linear_model.LinearRegression.\n",
    "    \n",
    "Plot the data (as scatterplot) and the fit (as lineplot) on a single seaborn chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91dece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [0.2, 0.52],\n",
    "    [0.5, 1.36],\n",
    "    [0.9, 1.00],\n",
    "    [1.3, 1.69],\n",
    "    [1.5, 2.58],\n",
    "    [2.8, 2.34]\n",
    "])\n",
    "\n",
    "\n",
    "def perform_least_squares(data):\n",
    "    x, y = data[:, 0], data[:, 1]\n",
    "    \n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "    \n",
    "    return theta_0, theta_1\n",
    "\n",
    "\n",
    "theta_0, theta_1 = perform_least_squares(data)\n",
    "\n",
    "min_x = min(data[:, 0]) * 0.9\n",
    "max_x = max(data[:, 0]) * 1.1\n",
    "x = np.linspace(min_x, max_x, 100)\n",
    "y = theta_1 * x + theta_0\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(data[:, 0].reshape(-1, 1), data[:, 1].flatten())\n",
    "y_lin = reg.predict(x.reshape(-1, 1))\n",
    "\n",
    "fig = plt.figure(tight_layout=True)\n",
    "fig.set_size_inches(16, 9)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(x=data[:, 0], y=data[:, 1], ax=ax1, label=\"data\")\n",
    "sns.lineplot(x=x, y=y, ax=ax1, label=\"your fit\")\n",
    "sns.lineplot(x=x, y=y_lin, ax=ax1, label=\"sklearn fit\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-little",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-crime",
   "metadata": {},
   "source": [
    "Generate negative interactions - copy your code from notebook 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg_per_pos = 5\n",
    "interactions_pos_neg_df = interactions_df[['user_id', 'item_id']].copy()\n",
    "\n",
    "# Indicate positive interactions\n",
    "\n",
    "interactions_pos_neg_df['interacted'] = 1\n",
    "\n",
    "# Generate negative interactions\n",
    "\n",
    "negative_interactions = []\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc368e",
   "metadata": {},
   "source": [
    "**Task 2.** From interactions_pos_neg_df create two dictionaries:\n",
    "- interactions_pos_neg_user_dict where keys are user ids and values are lists of item ids the user has interacted with,\n",
    "- interactions_pos_neg_item_dict where keys are item ids and values are lists of user ids who interacted with the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc211e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "print(\"Interactions of user_id=0\")\n",
    "print(interactions_pos_neg_user_dict[0])\n",
    "display(interactions_pos_neg_df.loc[interactions_pos_neg_df['user_id'] == 0])\n",
    "print()\n",
    "print(\"Interactions of user_id=1\")\n",
    "print(interactions_pos_neg_user_dict[1])\n",
    "display(interactions_pos_neg_df.loc[interactions_pos_neg_df['user_id'] == 1])\n",
    "print()\n",
    "print(\"Interactions of item_id=0\")\n",
    "print(interactions_pos_neg_item_dict[0])\n",
    "display(interactions_pos_neg_df.loc[interactions_pos_neg_df['item_id'] == 0])\n",
    "print()\n",
    "print(\"Interactions of item_id=1\")\n",
    "print(interactions_pos_neg_item_dict[1])\n",
    "display(interactions_pos_neg_df.loc[interactions_pos_neg_df['item_id'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-virgin",
   "metadata": {},
   "source": [
    "Initialize user and item embeddings of size embedding_dim - use your code from notebook 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-profession",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "print(user_repr_matrix)\n",
    "print()\n",
    "print(item_repr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-soundtrack",
   "metadata": {},
   "source": [
    "**Task 3.** Write the perform_mf_als_step method which takes item/user representations interacting_reprs for items the user has interacted with or for users who interacted with a given item (a 2D matrix where each row corresponds to an item representation/embedding), interaction values r_ui which is a 1D matrix of binary values to be predicted (the value from the interaction matrix - interactions_pos_neg_df), regularization constant reg_l, and performs a single step of Alternating Least Squares for matrix factorization as described in Koren, Bell, Volinksy \"Matrix Factorization Techniques for Recommender Systems\". You can use sklearn.linear_model.Ridge for that (x should be interacting_reprs, y should be r_ui and the result will be the new representation for the given user/item, you also have to disable fitting the intercept/bias in the model by setting fit_intercept=False). The method should return the new representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mf_als_step(interacting_reprs, r_ui, reg_l):\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "        \n",
    "    \n",
    "    return new_repr\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "interacting_item_repr_matrix = np.array([[0.75, 0.15], [-0.12, 0.32], [0.44, -0.95]])\n",
    "r_ui = np.array([1, 0, 0])\n",
    "reg_l = 0.1\n",
    "new_user_repr = perform_mf_als_step(interacting_item_repr_matrix, r_ui, reg_l)\n",
    "print(new_user_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-sword",
   "metadata": {},
   "source": [
    "**Task 4.** Write the perform_mf_als_epoch method which takes interactions_df, user_repr_matrix, item_repr_matrix, regularization constant reg_l as input, and then:\n",
    "- iterate over all users of interactions_df, perform perform_mf_als_step for every user (use the interactions_pos_neg_user_dict dictionary to get representations (rows in item_repr_matrix) and interactions (\"interacted\" column from interactions_df) for the items the the user has interacted with), update the appropriate user representation in the user_repr_matrix,\n",
    "- iterate over all items of interactions_df, perform perform_mf_als_step for every item (use the interactions_pos_neg_item_dict dictionary to get representations (rows in user_repr_matrix) and interactions (\"interacted\" column from interactions_df) for the users who interacted with the given item), update the appropriate item representation in the item_repr_matrix,\n",
    "- return updated user_repr_matrix, item_repr_matrix.\n",
    "\n",
    "To obtain consistent results run the cell creating base user and item representations again before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-upset",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def perform_mf_als_epoch(interactions_df, user_repr_matrix, item_repr_matrix, reg_l):\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "    \n",
    "    \n",
    "    return user_repr_matrix, item_repr_matrix\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "user_repr_matrix, item_repr_matrix \\\n",
    "    = perform_mf_als_epoch(interactions_pos_neg_df, user_repr_matrix, item_repr_matrix, reg_l)\n",
    "\n",
    "print(user_repr_matrix)\n",
    "print()\n",
    "print(item_repr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-velvet",
   "metadata": {},
   "source": [
    "**Task 5.** Write the perform_mf_als_training method which takes interactions_df, user_repr_matrix, item_repr_matrix, regularization constant reg_l, epsilon as input and performs the following steps until the loss change between epochs is smaller than epsilon for three consecutive epochs:\n",
    "- perform perform_mf_als_epoch, \n",
    "- calculate the loss for the user and item representations returned by perform_mf_als_epoch using the formula from the Koren, Bell, Volinksy \"Matrix Factorization Techniques for Recommender Systems\" paper.\n",
    "Finally return a tuple containing user_repr_matrix, item_repr_matrix and the final loss.\n",
    "\n",
    "To obtain consistent results run the cell creating base user and item representations and task 4 cell again before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-recording",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "def perform_mf_als_training(interactions_df, user_repr_matrix, item_repr_matrix, reg_l, epsilon):\n",
    "    liveloss = PlotLosses()\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "        \n",
    "\n",
    "        # Save and print epoch losses (this should be at the end of the loop over epochs)\n",
    "\n",
    "        training_last_avg_loss = total_loss / len(interactions_df)\n",
    "\n",
    "        if epoch >= 3: # A bound on epoch prevents showing extremely high losses in the first epochs\n",
    "            logs = {'loss': training_last_avg_loss}\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "        epoch += 1\n",
    "\n",
    "    return user_repr_matrix, item_repr_matrix, training_last_avg_loss\n",
    "\n",
    "# Test\n",
    "\n",
    "epsilon = 0.01\n",
    "user_repr_matrix, item_repr_matrix, training_last_avg_loss \\\n",
    "    = perform_mf_als_training(interactions_pos_neg_df, user_repr_matrix, item_repr_matrix, reg_l, epsilon)\n",
    "\n",
    "print(user_repr_matrix)\n",
    "print()\n",
    "print(item_repr_matrix)\n",
    "print()\n",
    "print(training_last_avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-tennis",
   "metadata": {},
   "source": [
    "### Plot movie representations\n",
    "\n",
    "Remember that they don't have to be good as only two dimensions have been used. But still try to find if you can assign any meaning to those dimensions based on your knowledge about plotted movies. You can open the image in new tab and enlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(tight_layout=True)\n",
    "fig.set_size_inches(64, 36)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(x=item_repr_matrix[:, 0], y=item_repr_matrix[:, 1], ax=ax1, color='red')\n",
    "sns.scatterplot(x=user_repr_matrix[:, 0], y=user_repr_matrix[:, 1], ax=ax1, color='blue')\n",
    "\n",
    "for i in range(len(item_repr_matrix)):\n",
    "    title = ml_movies_df.loc[ml_movies_df['item_id'] == item_id_reverse_mapping[i], 'title'].iloc[0]\n",
    "    plt.text(x=item_repr_matrix[i, 0] + 1 / 500, y=item_repr_matrix[i, 1] + 1 / 500, s=title, \n",
    "             fontdict=dict(color='red', size=8))\n",
    "    \n",
    "for i in range(len(user_repr_matrix)):\n",
    "    plt.text(x=user_repr_matrix[i, 0] + 1 / 500, y=user_repr_matrix[i, 1] + 1 / 500, s=i, \n",
    "             fontdict=dict(color='blue', size=8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70349c03",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation for binary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c3ac5",
   "metadata": {},
   "source": [
    "Generate the true logistic model and random data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_curve(x, theta_0, theta_1):\n",
    "    return 1 / (1 + np.exp(-theta_0 - theta_1 * x))\n",
    "\n",
    "real_theta_0 = -10\n",
    "real_theta_1 = 1.5\n",
    "\n",
    "data_x = rng.uniform(0, 10, size=20)\n",
    "p = log_curve(data_x, real_theta_0, real_theta_1)\n",
    "data_y = rng.binomial(1, p)\n",
    "\n",
    "min_x = min(data_x) * 0.9\n",
    "max_x = max(data_x) * 1.1\n",
    "x_grid = np.linspace(min_x, max_x, 100)\n",
    "y_log_curve = log_curve(x_grid, real_theta_0, real_theta_1)\n",
    "\n",
    "fig = plt.figure(tight_layout=True)\n",
    "fig.set_size_inches(16, 9)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(x=data_x, y=data_y, ax=ax1, label=\"data\")\n",
    "sns.lineplot(x=x_grid, y=y_log_curve, ax=ax1, label=\"true logistic curve\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd1375",
   "metadata": {},
   "source": [
    "**Task 6.** Code a loss function which returns the negative likelihood of observing a given dataset (data_x, data_y) given that the true model is a logistic curve with parameters theta_0, theta_1:\n",
    "$$P(y = 1) = \\frac{1}{1 + e^{-\\theta_0 - \\theta_1 * x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mle(params):\n",
    "    theta_0, theta_1 = params['theta_0'], params['theta_1']\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "    \n",
    "    return -likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb58cb1",
   "metadata": {},
   "source": [
    "Use hyperopt to minimize the loss defined in the previous task on data_x, data_y. Plot the data, the true model and the MLE fit on one seaborn chart. How does it compare to the MSE fit in the next cell? Which one looks better? Experiment with different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54755f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import traceback\n",
    "\n",
    "space = {\"theta_0\": hp.uniform(\"theta_0\", -50, 50),\n",
    "         \"theta_1\": hp.loguniform(\"theta_1\", np.log(0.01), np.log(100.0))}\n",
    "\n",
    "succeded = False\n",
    "n_tries = 3\n",
    "t = 0\n",
    "best_mle_param_set = {'theta_0': None, 'theta_1': None}\n",
    "while not succeded and t < n_tries:\n",
    "    try:\n",
    "        trials = Trials()\n",
    "        best_mle_param_set = fmin(loss_mle, space=space, algo=tpe.suggest, max_evals=100, show_progressbar=True, trials=trials)\n",
    "        succeded = True\n",
    "    except:\n",
    "        t += 1\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Best params\n",
    "\n",
    "print(\"theta_0 = {}\".format(best_mle_param_set['theta_0']))\n",
    "print(\"theta_1 = {}\".format(best_mle_param_set['theta_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_curve_mle_fit = log_curve(x_grid, best_mle_param_set['theta_0'], best_mle_param_set['theta_1'])\n",
    "\n",
    "fig = plt.figure(tight_layout=True)\n",
    "fig.set_size_inches(16, 9)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(x=data_x, y=data_y, ax=ax1, label=\"data\")\n",
    "sns.lineplot(x=x_grid, y=y_log_curve, ax=ax1, label=\"true logistic curve\")\n",
    "sns.lineplot(x=x_grid, y=y_log_curve_mle_fit, ax=ax1, label=\"mle fit logistic curve\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978509c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import traceback\n",
    "\n",
    "def loss_mse(params):\n",
    "    theta_0, theta_1 = params['theta_0'], params['theta_1']\n",
    "    mse = np.sum(np.power(data_y - log_curve(data_x, theta_0, theta_1), 2))\n",
    "    return mse\n",
    "\n",
    "space = {\"theta_0\": hp.uniform(\"theta_0\", -50, 50),\n",
    "         \"theta_1\": hp.loguniform(\"theta_1\", np.log(0.01), np.log(100.0))}\n",
    "\n",
    "succeded = False\n",
    "n_tries = 3\n",
    "t = 0\n",
    "best_mse_param_set = {'theta_0': None, 'theta_1': None}\n",
    "while not succeded and t < n_tries:\n",
    "    try:\n",
    "        trials = Trials()\n",
    "        best_mse_param_set = fmin(loss_mse, space=space, algo=tpe.suggest, max_evals=100, show_progressbar=True, trials=trials)\n",
    "        succeded = True\n",
    "    except:\n",
    "        t += 1\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Best params\n",
    "\n",
    "print(\"theta_0 = {}\".format(best_mse_param_set['theta_0']))\n",
    "print(\"theta_1 = {}\".format(best_mse_param_set['theta_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_curve_mse_fit = log_curve(x_grid, best_mse_param_set['theta_0'], best_mse_param_set['theta_1'])\n",
    "\n",
    "fig = plt.figure(tight_layout=True)\n",
    "fig.set_size_inches(16, 9)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(x=data_x, y=data_y, ax=ax1, label=\"data\")\n",
    "sns.lineplot(x=x_grid, y=y_log_curve, ax=ax1, label=\"true logistic curve\")\n",
    "sns.lineplot(x=x_grid, y=y_log_curve_mse_fit, ax=ax1, label=\"mse fit logistic curve\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
